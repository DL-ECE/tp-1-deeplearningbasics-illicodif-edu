{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "colab": {
      "name": "Rapport.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swBmZSvwX6E-"
      },
      "source": [
        "# TP-1 DLBasics\n",
        "\n",
        "## Digit classification using the MNIST dataset\n",
        "\n",
        "In this notebook you will train your first neural network. Feel free to look back at the Lecture-1 slides to complete the cells below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw9sWTSDX6E_"
      },
      "source": [
        "#### Install dependencies freeze by poetry \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zUWbdTzX6FA",
        "outputId": "78a435bf-f1fa-400b-9313-8581f32d80d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install matplotlib numpy scikit-learn==0.23.2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 10.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 9.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 9.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 9.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 9.5MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 9.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 9.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 9.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 9.5MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 204kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 215kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 225kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 245kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 409kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 419kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 430kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 440kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 450kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 460kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 481kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 491kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 501kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 624kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 634kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 645kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 655kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 665kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 675kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 686kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 696kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 716kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 727kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 737kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 747kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 757kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 808kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 819kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 829kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 839kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 860kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 870kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 880kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 890kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 901kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 911kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 921kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 931kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 952kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 962kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 972kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 983kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 993kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 9.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.2.4\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (0.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3qAjGJBX6FD"
      },
      "source": [
        "#### Import the different module we will need in this notebook \n",
        "\n",
        "All the dependencies are installed. Below we import them and will be using them in all our notebooks.\n",
        "\n",
        "Please feel free to look arround and look at their API. \n",
        "\n",
        "The student should be limited to these imports to complete this work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_en2qp-X6FD"
      },
      "source": [
        "# We import some python standard librairy utility function \n",
        "# see the [python doc](https://docs.python.org/3.6/library/functools.html?highlight=func#module-functools) for more info \n",
        "from functools import reduce \n",
        "import random \n",
        "\n",
        "# To create some plot and figures: matplolib [matplotlib doc](https://matplotlib.org/)\n",
        "# To do compute on matrix and vectors: [numpy doc](https://numpy.org/)\n",
        "# To do some classical Machine Learning: [sklearn doc](https://scikit-learn.org/stable/index.html)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMqVJGkxX6FG"
      },
      "source": [
        "# In order to have some reproducable results and easier debugging \n",
        "# we fix the seed of random.\n",
        "random.seed(1342)\n",
        "np.random.seed(1342)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDFEvmoaX6FJ"
      },
      "source": [
        "## Data preparation (3 pts)\n",
        "\n",
        "As seen in the lecture one of the earlier use case for deep learning was digit recognition. \n",
        "\n",
        "The dataset we will use today is the MNISTdataset http://yann.lecun.com/exdb/mnist/. \n",
        "\n",
        "One image will be represented a vector (a 28x28 image will be represented as vector with 784 entries).\n",
        "\n",
        "Thus, we will end up with a n_examples x 784 matrix to represent the images in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNlvaefNX6FK"
      },
      "source": [
        "mnist_data, mnist_target = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j91UiCWtX6FM"
      },
      "source": [
        "# Let's warmup and answer this first question\n",
        "# Replace the None with you answer.\n",
        "\n",
        "# How many image are in this dataset ? \n",
        "def data_length(dataset: np.array, target: np.array):\n",
        "    \"\"\"Function to compute the length of the dataset and the length of the target labels.\"\"\"\n",
        "    dataset_length = dataset.shape[0]\n",
        "    target_length = target.shape[0] \n",
        "    return dataset_length, target_length"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziLf9agSX6FO"
      },
      "source": [
        "# Let's look at on image from this dataset \n",
        "def plot_one_image(dataset: np.array, target: np.array, image_index: int):\n",
        "    \"\"\"Function to plot the image at the given index.\"\"\"\n",
        "    image = dataset[image_index].reshape(28,28)\n",
        "    target = target[image_index]\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"This is a {target}\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txtESCMQX6FQ"
      },
      "source": [
        "\n",
        "# In a similar fashion to classical machine learning, we will create a test split to known if the neural network is learning well.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist_data, mnist_target, test_size=0.33, random_state=1342)\n",
        "\n",
        "# You the 2 function below to check if they are working properly on this divided dataset.\n",
        "\n",
        "X_train_length, y_train_length = data_length(X_train, y_train)\n",
        "X_test_length, y_test_length = data_length(X_test, y_test)\n",
        "\n",
        "assert X_train_length == y_train_length and X_train_length == 46900\n",
        "assert X_test_length == y_test_length and X_test_length == 23100\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQXw9vdfX6FY"
      },
      "source": [
        "# It's important to normalize the data before feeding it into the neural network\n",
        "def normalize_data(dataset: np.array) -> np.array:\n",
        "    normalized_dataset = dataset / 255\n",
        "    return normalized_dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3YFobyX6Fb"
      },
      "source": [
        "It's also important to find a good representation of the target.\n",
        "\n",
        "In this notebook it will be one-hot vector. \n",
        "\n",
        "Complete the below function to turn the target vector into a one-hot matrix.\n",
        "\n",
        "For example, a `[0,1,9]` vector will become the following matrix:\n",
        "\n",
        "`[[1,0,0,0,0,0,0,0,0,0],\n",
        "  [0,1,0,0,0,0,0,0,0,0],\n",
        "  [0,0,0,0,0,0,0,0,0,1]]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2qa08w-X6Fb"
      },
      "source": [
        "def target_to_one_hot(target: np.array) -> np.array:\n",
        "    one_hot_matrix = np.zeros((target.shape[0], 10))\n",
        "    target = target.astype(int)\n",
        "    one_hot_matrix[np.arange(len(target)), target] = 1\n",
        "    return one_hot_matrix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmYKrhEiX6Fd"
      },
      "source": [
        "## Useful functions (3 pts)\n",
        "\n",
        "Implement the sigmoid function, its derivative and the softmax function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxurV9vpX6Fe"
      },
      "source": [
        "def sigmoid(M: np.array) -> np.array:\n",
        "    \"\"\"Apply a sigmoid to the input array\"\"\"\n",
        "    sig = (1/(1+np.exp(-M)))\n",
        "    return sig"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atEBTdsgX6Fg"
      },
      "source": [
        "def d_sigmoid(M: np.array)-> np.array:\n",
        "    \"\"\"Compute the derivative of the sigmoid\"\"\" \n",
        "    d_sig = (sigmoid(M) * (1-sigmoid(M)))\n",
        "    return d_sig"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlCuhBRdX6Fi"
      },
      "source": [
        "def softmax(X: np.array)-> np.array:\n",
        "    \"\"\"Apply a softmax to the input array\"\"\"\n",
        "    X_exp=np.exp(X)\n",
        "    X_sum=np.sum(X_exp,axis=1).reshape(-1,1)\n",
        "    return X_exp/X_sum\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LdT3tMoX6Fk"
      },
      "source": [
        "## Feed forward NN\n",
        "\n",
        "Now that the data is prepared it's time to create a neural network to learn on this dataset.\n",
        "\n",
        "You can look back at the lecture slides and need to replace the None in the below function in order to have the building blocks of this first neural network. \n",
        "\n",
        "To do so we are now going to create the FFNN class. It will take list of integers to represent the network.\n",
        "\n",
        "One element in the list corresponds to the number of neurones in the layer.\n",
        "`config = [784, 3, 4, 10]` will be an acceptable config: \n",
        "- inputs are 1x784 vectors \n",
        "- the model output should be a vector of size 10 to classify between 10 classes.\n",
        "- in the middle the hidden layer are fully customizable\n",
        "\n",
        "You have to do some implementations and replace the None assignment (variable = None). Do not do it for the Layer object.\n",
        "\n",
        "Warning: None return type for some methods are not supposed to be affected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS49WXIJX6Fl"
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "        self.W = None\n",
        "        self.D = None\n",
        "        self.F = None\n",
        "        self.activation = None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSAlF4jwX6Fn"
      },
      "source": [
        "class FFNN:\n",
        "    def __init__(self, config, minibatch_size=100, learning_rate=0.1):\n",
        "        self.layers = []\n",
        "        self.nlayers = len(config)\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        input_data = Layer()\n",
        "        # TODO: initialize the Z matrix with the a matrix containing only zeros\n",
        "        # its shape should be (minibatch_size, config[0])\n",
        "        input_data.Z = np.zeros((minibatch_size, config[0]), dtype=int)\n",
        "        self.layers.append(input_data)\n",
        "                                        \n",
        "        for i in range(1, len(config)):\n",
        "            nnodes = config[i]\n",
        "            layer  = Layer()\n",
        "            nlines_prev, ncols_prev = self.layers[i - 1].Z.shape\n",
        "            # TODO: initilize the weight matrix W in the layer with a random normal distribution\n",
        "            # its shape should be (ncols_prev, nnodes)\n",
        "            layer.W = np.random.normal(0,1, size=(ncols_prev, nnodes))\n",
        "            # TODO: initilize the matrix Z in the layer with a matrix containing only zeros\n",
        "            # its shape should be (nlines_prev, nnodes)\n",
        "            layer.Z = np.zeros((nlines_prev, nnodes))\n",
        "            # TODO: use the sigmoid activation function\n",
        "            layer.activation = sigmoid\n",
        "            self.layers.append(layer)\n",
        "        # TODO: Your last layer activation should be a softmax\n",
        "        self.layers[-1].activation = softmax\n",
        "        \n",
        "    def one_step_forward(self, signal: np.array, cur_layer: Layer)-> np.array:\n",
        "        # Compute the F and Z matrix for the current layer and return Z\n",
        "        \n",
        "        # TODO: Compute the dot product betzeen the signal and the current layer W matrix\n",
        "        S = np.dot(signal, cur_layer.W)\n",
        "        # TODO: Compute the F matrix of the current layer\n",
        "        cur_layer.F = d_sigmoid(S).transpose()\n",
        "        # Compute the activation od the current layer\n",
        "        cur_layer.Z = cur_layer.activation(S)\n",
        "        return cur_layer.Z\n",
        "       \n",
        "    def forward_pass(self, input_data: np.array)-> np.array:\n",
        "        # TODO: perform the whole forward pass using the on_step_forward function\n",
        "        self.layers[0].Z = input_data\n",
        "        for i in range(1, self.nlayers):\n",
        "          self.layers[i].Z = self.one_step_forward(self.layers[i-1].Z, self.layers[i])\n",
        "  \n",
        "        return self.layers[-1].Z\n",
        "\n",
        "    \n",
        "    def one_step_backward(self, prev_layer: Layer, cur_layer: Layer)-> Layer:\n",
        "        # TODO: Compute the D matrix of the current layer using the previous layer and return the current layer\n",
        "        Di = cur_layer.F * np.dot(prev_layer.W, prev_layer.D)\n",
        "        cur_layer.D = Di\n",
        "        return cur_layer\n",
        "        \n",
        "    def backward_pass(self, D_out: np.array)-> None:\n",
        "        self.layers[-1].D = D_out.T\n",
        "        # TODO: Compute the D matrix for all the layers (excluding the first one which corresponds to the input itself)\n",
        "        # (you should only use self.layers[1:])\n",
        "        for i in range(len(self.layers[1:])-1, 0, -1):\n",
        "          self.layers[i] = self.one_step_backward(self.layers[i+1], self.layers[i])\n",
        "        \n",
        "    \n",
        "    def update_weights(self, cur_layer: Layer, next_layer: Layer)-> Layer:\n",
        "        # TODO: Update the W matrix of the next_layer using the current_layer and the learning rate\n",
        "        # and return the next_layer\n",
        "        next_layer.W = next_layer.W - self.learning_rate * ((np.dot(next_layer.D, cur_layer.Z)).T)\n",
        "        return next_layer\n",
        "    \n",
        "    def update_all_weights(self)-> None:\n",
        "        # TODO: Update all W matrix using the update_weights function\n",
        "        for i in range(0, (self.nlayers) - 1):\n",
        "          self.layers[i+1] = self.update_weights(self.layers[i], self.layers[i+1])\n",
        "        \n",
        "    def get_error(self, y_pred: np.array, y_batch: np.array)-> float:\n",
        "        # TODO: return the accuracy on the predictions\n",
        "        # the accuracy should be in the [0.0, 1.0] range\n",
        "        sum = 0\n",
        "        for i in range(0, len(y_pred)):\n",
        "          if (np.argmax(y_pred[i]) == np.argmax(y_batch[i])):\n",
        "            sum += 1\n",
        "        return sum/len(y_pred)\n",
        "    \n",
        "    def get_test_error(self, X: np.array, y: np.array)-> float:\n",
        "        # TODO: Compute the accuracy using the get_error function\n",
        "        # use np.argmax, retourne l'index de la valeur max du vecteur\n",
        "        nbatch = X.shape[0]\n",
        "        error_sum = 0.0\n",
        "        for i in range(0, nbatch):\n",
        "            X_batch = X[i,:,:].reshape(self.minibatch_size, -1)\n",
        "            y_batch = y[i,:,:].reshape(self.minibatch_size, -1)           \n",
        "            # TODO: get y_pred using the forward pass\n",
        "            error_sum += self.get_error(self.forward_pass(X_batch), y_batch)\n",
        "        return error_sum / nbatch\n",
        "            \n",
        "        \n",
        "    def train(self, nepoch, X_train, y_train, X_test, y_test)-> float:\n",
        "        X_train = X_train.reshape(-1, self.minibatch_size, 784)\n",
        "        y_train = y_train.reshape(-1, self.minibatch_size, 10)\n",
        "        \n",
        "        X_test = X_test.reshape(-1, self.minibatch_size, 784)\n",
        "        y_test = y_test.reshape(-1, self.minibatch_size, 10)\n",
        "        \n",
        "        # TODO: Get the number of batch based on X_train's shape\n",
        "        nbatch = X_train.shape[0]\n",
        "        error_test = 0.0\n",
        "        for epoch in range(0, nepoch):\n",
        "            error_sum_train = 0.0\n",
        "            for i in range(0, nbatch):\n",
        "                X_batch = X_train[i,:, :]\n",
        "                y_batch = y_train[i,:, :]\n",
        "        \n",
        "                y_pred = self.forward_pass(X_batch)\n",
        "                self.backward_pass(y_pred - y_batch)\n",
        "                self.update_all_weights()\n",
        "                error_sum_train += self.get_error(y_pred, y_batch)\n",
        "            error_test = self.get_test_error(X_test, y_test)\n",
        "            print(f\"Training accuracy: {error_sum_train / nbatch:.3f}, Test accuracy: {error_test:.3f}\")\n",
        "        return error_test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5mghxmX6Fp"
      },
      "source": [
        "## Training phase (12 pts)\n",
        "\n",
        "Now, it is time to train the model !!\n",
        "\n",
        "You can play with the different parameters (minibatch_size, nepoch, learning_rate and the number of hidden layers)\n",
        "\n",
        "It's on 12 points because there is a lot of functions to fill but also we want the training best training accuracy. \n",
        "\n",
        "To have all the point your neural network needs to have a Test accuracy > 92 % !! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUB3ysbzX6Fp"
      },
      "source": [
        "minibatch_size = 50\n",
        "nepoch = 15\n",
        "learning_rate = 0.01\n",
        "\n",
        "ffnn = FFNN(config=[784, 255, 255, 10], minibatch_size=minibatch_size, learning_rate=learning_rate)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mke_xy9pX6Fr",
        "outputId": "e5b703e2-d5f3-4a13-9a8b-6f955a4a7573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "assert X_train.shape[0] % minibatch_size == 0\n",
        "assert X_test.shape[0] % minibatch_size == 0\n",
        "\n",
        "X_train_norm = normalize_data(X_train)\n",
        "X_test_norm = normalize_data(X_test)\n",
        "y_train_hot = target_to_one_hot(y_train)\n",
        "y_test_hot = target_to_one_hot(y_test)\n",
        "\n",
        "err = ffnn.train(nepoch, X_train_norm, y_train_hot, X_test_norm, y_test_hot)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.799, Test accuracy: 0.874\n",
            "Training accuracy: 0.906, Test accuracy: 0.904\n",
            "Training accuracy: 0.935, Test accuracy: 0.917\n",
            "Training accuracy: 0.953, Test accuracy: 0.921\n",
            "Training accuracy: 0.964, Test accuracy: 0.925\n",
            "Training accuracy: 0.972, Test accuracy: 0.925\n",
            "Training accuracy: 0.978, Test accuracy: 0.926\n",
            "Training accuracy: 0.984, Test accuracy: 0.928\n",
            "Training accuracy: 0.988, Test accuracy: 0.931\n",
            "Training accuracy: 0.991, Test accuracy: 0.933\n",
            "Training accuracy: 0.994, Test accuracy: 0.934\n",
            "Training accuracy: 0.995, Test accuracy: 0.935\n",
            "Training accuracy: 0.996, Test accuracy: 0.936\n",
            "Training accuracy: 0.997, Test accuracy: 0.937\n",
            "Training accuracy: 0.998, Test accuracy: 0.937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfmuRc8lX6Ft"
      },
      "source": [
        "## Error analysis (2 pts)\n",
        "\n",
        "Here we use a subset of the test data to try and find some miss classification.\n",
        "\n",
        "It will help us understand why the neural network failed sometimes to classify images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vd3CnRYX6Fu",
        "outputId": "8fc22013-1764-48e8-bfed-6ea517ef7701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "nsample = 1000\n",
        "X_demo = X_test[:nsample]\n",
        "y_demo = ffnn.forward_pass(X_demo)\n",
        "y_true = y_test[:nsample]\n",
        "\n",
        "index_to_plot = 45\n",
        "plot_one_image(X_demo, y_true, index_to_plot)\n",
        "\n",
        "# Compare to the prediction \n",
        "prediction = np.argmax(y_demo[index_to_plot])\n",
        "true_target = np.argmax(y_true[index_to_plot])\n",
        "\n",
        "# is it the same number ? \n",
        "print(\"The prediction is \" + str(prediction))\n",
        "print(\"The true target is \" + str(true_target))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction is 4\n",
            "The true target is 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU3PcTNXX6Fw",
        "outputId": "d091bd0a-b2ac-46b6-813c-21bf1acb129e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loop arround the demo test set and try to find a miss prediction\n",
        "miss_pred = 0\n",
        "for i in range(0, nsample):   \n",
        "    prediction = str(np.argmax(y_demo[i])) # Todo\n",
        "    true_target = y_true[i] # Todo\n",
        "    if prediction != true_target:\n",
        "        # TODO. ADD PASS HERE IF BUG IN AUTOCORRECT\n",
        "        miss_pred += 1\n",
        "        pass\n",
        "        \n",
        "print(miss_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHFIP_KwX6Fy"
      },
      "source": [
        "## Open analysis\n",
        "\n",
        "in the cell below please explain you choice for all the parameters of your configuration: \n",
        "\n",
        "- minibatch_size\n",
        "- nepoch\n",
        "- config\n",
        "- learning_rate\n",
        "\n",
        "Also explain how the neural network behave when changing them ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhAKCKoRX6Fy"
      },
      "source": [
        "## Open analysis answer\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFUh7sQZUS-M"
      },
      "source": [
        "## nepoch : j'ai d'abord essayé avec 10, puis 20. J'ai ensuite remarqué que la progression était beaucoup plus lente à partir de 15 j'ai donc choisi ce paramètre afin de supprimer les itérations inutiles.\n",
        "## Ce paramètre est le nombre d'entraînement que va réaliser le réseau \n",
        "##\n",
        "## learning_rate : j'ai effectué de nombreux tests et j'ai remarqué que mes résultats étaient généralement moins bons avec un learning rate > 0.01. J'ai donc gardé ma valeur de 0.01\n",
        "## Ce paramètre influence la rapidité avec laquelle nous allons modifier les poids de notre réseau en fonction de l'erreur. Mon learning_rate est assez faible je sais donc que j'ai donc choisi de ne garder les configurations que si je dépasse les 75% d'accuracy au premier training. \n",
        "##\n",
        "## minibatch_size : J'ai d'abord commencé par faire la liste de toutes les valeurs possibles (1, 2, 4, 5, 7, 10, 14, 20, 25, 28, 35, 50, 70, 100, 140, 175, 350, 700).\n",
        "## J'ai ensuite testé plusieurs valeurs avec des valeurs de config aléatoire. Afin de ne pas me perdre j'ai réalisé un tableau excel dans lequel j'ai noté mes paramètres ainsi que le pourcentage du test accuracy.\n",
        "## Ce paramètre permet de découper le trainning set en plus petit paquet lors de la phase de training. \n",
        "##\n",
        "## config : Dès que j'ai essayé une nouvelle valeur pour le minibatch_size, j'ai essayé 20, 50, 100, 255 pour la config. Si l'une des configurations me donne de bons résultats je vais alors essayer différentes valeurs proches de cette dernière.\n",
        "## Ce paramètre permet de sélectionner le nombre de neurones par couches "
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}